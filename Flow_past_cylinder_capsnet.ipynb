{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWnuqFe2o8x2",
        "outputId": "32c54d22-f259-411f-972c-57d92b217d20"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import h5py\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from dataclasses import dataclass\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import csv\n",
        "import math\n",
        "import pandas\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import scipy.io\n",
        "import h5py\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfs9h95yceT"
      },
      "source": [
        "f = h5py.File('/cylinder_data/cylinder_training.hdf5', 'r')\n",
        "g1 = f['250']\n",
        "g2 = f['275']\n",
        "g3 = f['325']\n",
        "pressure = np.concatenate((np.array(g1['p']),np.array(g2['p']),np.array(g3['p'])),axis=0)\n",
        "ux = np.concatenate((np.array(g1['ux']),np.array(g2['ux']),np.array(g3['ux'])),axis=0)\n",
        "uy = np.concatenate((np.array(g1['uy']),np.array(g2['uy']),np.array(g3['uy'])),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = c = np.stack((ux[:,5:55,:100],uy[:,5:55,:100]), axis=3)\n",
        "y_data = np.expand_dims(pressure[:,5:55,:100],axis=3)"
      ],
      "metadata": {
        "id": "hrK0KY4tqjwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset_creator: \n",
        "  def __init__(self, x_data, y_data, train_size,val_size,test_size):\n",
        "    self.x = x_data\n",
        "    self.y = y_data\n",
        "    self.tr_s = train_size\n",
        "    self.v_s = val_size\n",
        "    self.ts_s = test_size\n",
        "    self.dataset = tf.data.Dataset.from_tensor_slices((self.x, self.y)).shuffle(buffer_size=self.tr_s)\n",
        "\n",
        "  def split_dataset(self):\n",
        "    train_dataset = self.dataset.take(self.tr_s)\n",
        "    val_dataset = self.dataset.skip(self.tr_s)\n",
        "    test_dataset = val_dataset.skip(self.v_s)\n",
        "    val_dataset = val_dataset.take(self.v_s)\n",
        "    test_dataset = test_dataset.take(self.ts_s)\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "  def tupleunpack(self,dataset, size):\n",
        "    x1 = np.zeros((size,self.x.shape[1],self.x.shape[2],self.x.shape[-1]))\n",
        "    y  = np.zeros((size,self.y.shape[1],self.y.shape[2],self.y.shape[-1]))\n",
        "    for steps,(t1,t2) in enumerate(dataset):\n",
        "      x1[steps] = t1\n",
        "      y[steps] = t2\n",
        "    return x1,y\n",
        "\n",
        "  def get_data(self):\n",
        "    train_dataset, val_dataset, test_dataset = self.split_dataset()\n",
        "    x_train,y_train = self.tupleunpack(train_dataset, self.tr_s)\n",
        "    x_test,y_test = self.tupleunpack(test_dataset, self.ts_s)\n",
        "    x_val,y_val = self.tupleunpack(val_dataset, self.v_s)\n",
        "    return x_train,y_train,x_val,y_val,x_test,y_test"
      ],
      "metadata": {
        "id": "JN7YhvyRXjEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 1000\n",
        "test_size = 100\n",
        "val_size = 100"
      ],
      "metadata": {
        "id": "kITcx8BOxOJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(456)\n",
        "dataset = dataset_creator(x_data, y_data, train_size,val_size,test_size)\n",
        "x_train,y_train,x_test,y_test,x_val,y_val = dataset.get_data()"
      ],
      "metadata": {
        "id": "rEIm6-fY1wlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHeLw1do8x5"
      },
      "source": [
        "# Capsule Layers \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQti96GTo8x5"
      },
      "source": [
        "def squash(vectors, axis=-1):\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzCwLg9ExiLl"
      },
      "source": [
        "class CapsuleLayer(layers.Layer):\n",
        "  \n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        primaryout_expand = tf.expand_dims(tf.expand_dims(inputs, -1),2)\n",
        "        primaryout_tiled = tf.tile(primaryout_expand, [1, 1, self.num_capsule, 1, 1],name=\"caps1_output_tiled\")\n",
        "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=primaryout_tiled)\n",
        "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.input_num_capsule, self.num_capsule,1,1])\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "            predictions = tf.multiply(c,inputs_hat , name=\"weighted_predictions\")\n",
        "            w_sum = tf.reduce_sum(predictions, axis=1, keepdims=True, name=\"weighted_sum\")\n",
        "            outputs = squash(w_sum,axis=-2)\n",
        "            if i < self.routings - 1:\n",
        "                tiled_output = tf.tile(outputs, [1, self.input_num_capsule, 1, 1, 1]) \n",
        "                b += tf.matmul(inputs_hat_stopped, tiled_output, transpose_a=True)\n",
        "        return tf.squeeze(outputs,[1,4])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO70XnrSxl5u"
      },
      "source": [
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=\"glorot_uniform\", bias_initializer=\"zeros\", use_bias= False)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule])(output)\n",
        "    return layers.Lambda(squash)(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDdQcGoko8x8"
      },
      "source": [
        "# Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPoxIcj_WLF5"
      },
      "source": [
        "def encoder_decoder(input,input_shape, n_class, routings):\n",
        "    normalised = layers.BatchNormalization()(input)\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=(9, 17), strides=1, padding='valid', activation='relu')(normalised)\n",
        "    conv2 = layers.Conv2D(filters=128, kernel_size=(9, 17), strides=2, padding='valid', activation='relu')(conv1)\n",
        "    primarycaps = PrimaryCap(conv2, dim_capsule=8, n_channels=32, kernel_size=(9,17), strides=2, padding='valid')\n",
        "    digitcaps = CapsuleLayer(num_capsule = n_class, dim_capsule=16, routings = routings)(primarycaps)\n",
        "    reshape_layer = layers.Reshape(target_shape = [n_class*16])(digitcaps)\n",
        "    decoder_layer1 = layers.Dense(512, activation='relu')(reshape_layer)\n",
        "    decoder_layer3 = layers.Dense(np.prod(input_shape), activation=tf.keras.layers.LeakyReLU(alpha=0.3))(decoder_layer1)\n",
        "    decoder_layer4         = layers.Reshape(target_shape=input_shape)(decoder_layer3)\n",
        "    output = layers.Concatenate(axis=3)([input,decoder_layer4])\n",
        "    return output\n",
        "\n",
        "def decoder(input, input_shape):\n",
        "    normalised = layers.BatchNormalization()(input)\n",
        "    deconv1 = layers.Conv2DTranspose(filters = 3, kernel_size = (3,3), strides=1, padding='same', activation='relu', \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised)\n",
        "    normalised2 = layers.BatchNormalization()(deconv1)\n",
        "    deconv2 = layers.Conv2DTranspose(filters = 2, kernel_size = (3,3), strides=1, padding='same', activation='relu', \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised2)\n",
        "    normalised3 = layers.BatchNormalization()(deconv2)\n",
        "    deconv3 = layers.Conv2DTranspose(filters = 1, kernel_size = (3,3), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3), \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised3)\n",
        "    return deconv3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_czKxrkOA4c"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "  x = layers.Input(shape=input_shape)\n",
        "  encoded = encoder_decoder(input = x,input_shape = input_shape, n_class = n_class, routings = routings)\n",
        "  decoded = decoder(input = encoded, input_shape = input_shape)\n",
        "  train_model = models.Model(x,decoded)\n",
        "  eval_model = models.Model(x, decoded)\n",
        "  return train_model, eval_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrbM8k6misGf"
      },
      "source": [
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keyargs():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  \n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  \n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  \n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None: \n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZJyVmido8yK"
      },
      "source": [
        "def train(model, x_train,y_train,x_test,y_test, args):\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs', histogram_freq=int(args.debug))\n",
        "    !mkdir \"./result/vl_cp\"\n",
        "    !mkdir \"./result/tr_cp\"\n",
        "    filepath_vlcp = args.save_dir + \"/vl_cp/imp_vl.hdf5\"\n",
        "    filepath_trcp = args.save_dir + \"/tr_cp/imp_tr.hdf5\"\n",
        "    vl_cp = tf.keras.callbacks.ModelCheckpoint(filepath=filepath_vlcp, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "    tr_cp = tf.keras.callbacks.ModelCheckpoint(filepath=filepath_trcp, monitor='loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "                  loss=['mse'])\n",
        "\n",
        "    tic = time.time()\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[x_test, y_test], callbacks=[log, tb,vl_cp,tr_cp, lr_decay])\n",
        "    tic2 = time.time()\n",
        "\n",
        "    model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "    #plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZH7FbKHiNls"
      },
      "source": [
        "def test(model, x_test,y_test, args):\n",
        "    x_recon = model.predict(x_test, batch_size=100)\n",
        "    print('-'*30 + 'Begin: test' + '-'*30)\n",
        "    #print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
        "\n",
        "    img = combine_images(np.concatenate([y_test[:50],x_recon[:50]]))\n",
        "    image = img * 255\n",
        "    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
        "    print()\n",
        "    print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
        "    print('-' * 30 + 'End: test' + '-' * 30)\n",
        "    plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\"))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EKbfeEhiYkr",
        "outputId": "89e6f48e-e339-4b91-c0bf-3e15b35c4d11"
      },
      "source": [
        "parser = argparse.ArgumentParser(description=\"Capsule Network.\")\n",
        "parser.add_argument('--epochs', default=140, type=int)\n",
        "parser.add_argument('--batch_size', default=25, type=int)\n",
        "parser.add_argument('--lr', default=0.01,type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "parser.add_argument('--lr_decay', default=0.98, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "parser.add_argument('-r', '--routings', default=3, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result')\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=25, debug=False, digit=5, epochs=140, lam_recon=0.392, lr=0.01, lr_decay=0.98, routings=3, save_dir='./result', shift_fraction=0.1, testing=False, weights=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhecQ9aJXGV4"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=x_train.shape[1:], n_class = 30, routings = args.routings)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NCOZGHlo8yM"
      },
      "source": [
        "train(model=model,x_train = x_train, y_train = y_train, x_test = x_val, y_test = y_val, args=args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2, eval_model2 = CapsNet(input_shape=x_train.shape[1:], n_class = 30, routings = args.routings)\n",
        "model2.compile(optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "                  loss=['mse'])\n",
        "model2.load_weights('/content/result/vl_cp/imp_vl.hdf5')"
      ],
      "metadata": {
        "id": "20PHKbJYhDtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBWmeL1fA4NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0587aa02-11cd-4b74-bd17-abbb5f4575ef"
      },
      "source": [
        "x_recon1 = model2.predict(x_train, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 11s 42ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_recon2 = model2.predict(x_val, batch_size=50)"
      ],
      "metadata": {
        "id": "rE-Wsebk-wco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55460953-2691-46c4-d608-2aea2053eae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 43ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_recon3 = model2.predict(x_test, batch_size=50)"
      ],
      "metadata": {
        "id": "p21u9GV6-yQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7846ed54-c050-4015-d773-7fab2b3e0ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 43ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_val = model2.evaluate(x = x_val,y = y_val, batch_size=50)"
      ],
      "metadata": {
        "id": "aP1i7BqJ_AB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb24d01c-ab83-435a-8640-5031aed92155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 47ms/step - loss: 4.4130e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_test = model2.evaluate(x = x_test,y = y_test, batch_size=50)"
      ],
      "metadata": {
        "id": "4JT4sP7eAE39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d94dd80-31af-473c-b8c7-69b59765487a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 53ms/step - loss: 3.8623e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_train = model2.evaluate(x = x_train,y = y_train, batch_size=50)"
      ],
      "metadata": {
        "id": "RhZ_x8SPAH7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b96d46-9fe5-4e2c-a731-1fd0874d4523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 1s 67ms/step - loss: 4.1531e-04\n"
          ]
        }
      ]
    }
  ]
}