{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWnuqFe2o8x2",
        "outputId": "181bac31-79aa-423d-f260-f5de734b7120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "import h5py\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import math\n",
        "import argparse\n",
        "import pandas\n",
        "import os\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQfs9h95yceT"
      },
      "outputs": [],
      "source": [
        "with np.load('/rbf_mix_length_scales_samp_per_scale_50_size_2000_v_10.npz') as data:\n",
        "    x_orig = data['inputs']\n",
        "    y_orig = data['outputs']\n",
        "x_data = np.expand_dims(x_orig,axis=3)/(np.max(x_orig))\n",
        "y_data = np.expand_dims(y_orig,axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kITcx8BOxOJd"
      },
      "outputs": [],
      "source": [
        "train_size = 1000\n",
        "test_size = 500\n",
        "val_size = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r07kowdH7H4V"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(456)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
        "dataset = dataset.shuffle(buffer_size=train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWuThlD_7f1J"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "test_dataset = test_dataset.skip(test_size)\n",
        "val_dataset = test_dataset.take(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XRwWG0t7o8I"
      },
      "outputs": [],
      "source": [
        "def tupleunpack(dataset,number):\n",
        "  x1 = np.zeros((number,32,32,1))\n",
        "  y  = np.zeros((number,32,32,1))\n",
        "  for steps,(t1,t2) in enumerate(dataset):\n",
        "    x1[steps] = t1\n",
        "    y[steps] = t2\n",
        "  return x1,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49NvsPz27xEe"
      },
      "outputs": [],
      "source": [
        "x_train,y_train = tupleunpack(train_dataset,train_size)\n",
        "x_test,y_test = tupleunpack(test_dataset,test_size)\n",
        "x_val,y_val = tupleunpack(val_dataset,val_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHeLw1do8x5"
      },
      "source": [
        "# Capsule Layers \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQti96GTo8x5"
      },
      "outputs": [],
      "source": [
        "def squash(vectors, axis=-1):\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzCwLg9ExiLl"
      },
      "outputs": [],
      "source": [
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        primaryout_expand = tf.expand_dims(tf.expand_dims(inputs, -1),2)\n",
        "        primaryout_tiled = tf.tile(primaryout_expand, [1, 1, self.num_capsule, 1, 1],name=\"caps1_output_tiled\")\n",
        "\n",
        "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=primaryout_tiled)\n",
        "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
        "     \n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.input_num_capsule, self.num_capsule,1,1])\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "\n",
        "            c = tf.nn.softmax(b, axis=2)\n",
        "\n",
        "            predictions = tf.multiply(c,inputs_hat , name=\"weighted_predictions\")\n",
        "            w_sum = tf.reduce_sum(predictions, axis=1, keepdims=True, name=\"weighted_sum\")\n",
        "            outputs = squash(w_sum,axis=-2)\n",
        "            if i < self.routings - 1:\n",
        "\n",
        "                tiled_output = tf.tile(outputs, [1, self.input_num_capsule, 1, 1, 1]) \n",
        "                b += tf.matmul(inputs_hat_stopped, tiled_output, transpose_a=True)\n",
        "\n",
        "        return tf.squeeze(outputs,[1,4])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO70XnrSxl5u"
      },
      "outputs": [],
      "source": [
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=\"glorot_uniform\", bias_initializer=\"zeros\", use_bias= False)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule])(output)\n",
        "    return layers.Lambda(squash)(outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDdQcGoko8x8"
      },
      "source": [
        "# Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPoxIcj_WLF5"
      },
      "outputs": [],
      "source": [
        "def encoder_decoder(input,input_shape, n_class, routings):\n",
        "\n",
        "    normalised = layers.BatchNormalization()(input)\n",
        "\n",
        "    conv1 = layers.Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='valid', activation='relu')(normalised)\n",
        "    conv2 = layers.Conv2D(filters=128, kernel_size=(3, 3), strides=2, padding='valid', activation='relu')(conv1)\n",
        "\n",
        "    primarycaps = PrimaryCap(conv2, dim_capsule=8, n_channels=32, kernel_size=(3,3), strides=2, padding='valid')\n",
        "\n",
        "\n",
        "    capslayer = CapsuleLayer(num_capsule = n_class, dim_capsule=16, routings = routings)(primarycaps)\n",
        "\n",
        "\n",
        "\n",
        "    reshape_layer = layers.Reshape(target_shape = [n_class*16])(capslayer)\n",
        "    decoder_layer1 = layers.Dense(512, activation='relu')(reshape_layer)\n",
        "    #decoder_layer2 = layers.Dense(1024, activation='relu')(decoder_layer1)\n",
        "    decoder_layer3 = layers.Dense(np.prod(input_shape), activation=tf.keras.layers.LeakyReLU(alpha=0.3))(decoder_layer1)\n",
        "    decoder_layer4         = layers.Reshape(target_shape=input_shape)(decoder_layer3)\n",
        "    output = layers.Concatenate(axis=3)([input,decoder_layer4])\n",
        "    return output\n",
        "\n",
        "\n",
        "def decoder(input, input_shape):\n",
        "    normalised = layers.BatchNormalization()(input)\n",
        "    deconv1 = layers.Conv2DTranspose(filters = 3, kernel_size = (3,3), strides=1, padding='same', activation='relu', \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised)\n",
        "    normalised2 = layers.BatchNormalization()(deconv1)\n",
        "    deconv2 = layers.Conv2DTranspose(filters = 2, kernel_size = (3,3), strides=1, padding='same', activation='relu', \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised2)\n",
        "    normalised3 = layers.BatchNormalization()(deconv2)\n",
        "    deconv3 = layers.Conv2DTranspose(filters = 1, kernel_size = (3,3), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3), \n",
        "                                            use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros')(normalised3)\n",
        "    return deconv3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_czKxrkOA4c"
      },
      "outputs": [],
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "  x = layers.Input(shape=input_shape)\n",
        "  encoded1 = encoder_decoder(input = x,input_shape = input_shape, n_class = n_class, routings = routings)\n",
        "  encoded2 = encoder_decoder(input = encoded1,input_shape = input_shape, n_class = n_class, routings = routings)\n",
        "  decoded = decoder(input = encoded2, input_shape = input_shape)\n",
        "  train_model = models.Model(x,decoded)\n",
        "  eval_model = models.Model(x, decoded)\n",
        "  return train_model, eval_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrbM8k6misGf"
      },
      "outputs": [],
      "source": [
        "def plot_log(filename, show=True):\n",
        "\n",
        "    data = pandas.read_csv(filename)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,6))\n",
        "    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n",
        "    fig.add_subplot(211)\n",
        "    for key in data.keyargs():\n",
        "        if key.find('loss') >= 0 and not key.find('val') >= 0:  \n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training loss')\n",
        "\n",
        "    fig.add_subplot(212)\n",
        "    for key in data.keys():\n",
        "        if key.find('acc') >= 0:  \n",
        "            plt.plot(data['epoch'].values, data[key].values, label=key)\n",
        "    plt.legend()\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # fig.savefig('result/log.png')\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def combine_images(generated_images, height=None, width=None):\n",
        "    num = generated_images.shape[0]\n",
        "    if width is None and height is None:\n",
        "        width = int(math.sqrt(num))\n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif width is not None and height is None:  \n",
        "        height = int(math.ceil(float(num)/width))\n",
        "    elif height is not None and width is None:  \n",
        "        width = int(math.ceil(float(num)/height))\n",
        "\n",
        "    shape = generated_images.shape[1:3]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZJyVmido8yK"
      },
      "outputs": [],
      "source": [
        "def train(model, x_train,y_train,x_test,y_test, args):\n",
        "\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
        "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs', histogram_freq=int(args.debug))\n",
        "    !mkdir \"./result/vl_cp\"\n",
        "    !mkdir \"./result/tr_cp\"\n",
        "    filepath_vlcp = args.save_dir + \"/vl_cp/imp_vl.hdf5\"\n",
        "    filepath_trcp = args.save_dir + \"/tr_cp/imp_tr.hdf5\"\n",
        "    vl_cp = tf.keras.callbacks.ModelCheckpoint(filepath=filepath_vlcp, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "    tr_cp = tf.keras.callbacks.ModelCheckpoint(filepath=filepath_trcp, monitor='loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "                  loss=['mse'])\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs,\n",
        "              validation_data=[x_test, y_test], callbacks=[log, tb,vl_cp,tr_cp, lr_decay])\n",
        "    tic2 = time.time()\n",
        "\n",
        "    model.save_weights(args.save_dir + '/trained_model.h5')\n",
        "    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n",
        "    plot_log(args.save_dir + '/log.csv', show=True)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZH7FbKHiNls"
      },
      "outputs": [],
      "source": [
        "def test(model, x_test,y_test, args):\n",
        "    x_recon = model.predict(x_test, batch_size=100)\n",
        "    print('-'*30 + 'Begin: test' + '-'*30)\n",
        "    #print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
        "\n",
        "    img = combine_images(np.concatenate([y_test[:50],x_recon[:50]]))\n",
        "    image = img * 255\n",
        "    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"/real_and_recon.png\")\n",
        "    print()\n",
        "    print('Reconstructed images are saved to %s/real_and_recon.png' % args.save_dir)\n",
        "    print('-' * 30 + 'End: test' + '-' * 30)\n",
        "    plt.imshow(plt.imread(args.save_dir + \"/real_and_recon.png\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EKbfeEhiYkr"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description=\"Capsule Network on MNIST.\")\n",
        "parser.add_argument('--epochs', default=140, type=int)\n",
        "parser.add_argument('--batch_size', default=25, type=int)\n",
        "parser.add_argument('--lr', default=0.05,type=float,\n",
        "                        help=\"Initial learning rate\")\n",
        "parser.add_argument('--lr_decay', default=0.98, type=float,\n",
        "                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n",
        "parser.add_argument('-r', '--routings', default=3, type=int,\n",
        "                        help=\"Number of iterations used in routing algorithm. should > 0\")\n",
        "\n",
        "parser.add_argument('--debug', action='store_true',\n",
        "                        help=\"Save weights by TensorBoard\")\n",
        "parser.add_argument('--save_dir', default='./result')\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "if not os.path.exists(args.save_dir):\n",
        "    os.makedirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhecQ9aJXGV4"
      },
      "outputs": [],
      "source": [
        "model, eval_model = CapsNet(input_shape=x_train.shape[1:], n_class = 30, routings = args.routings)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NCOZGHlo8yM"
      },
      "outputs": [],
      "source": [
        "train(model=model,x_train = x_train, y_train = y_train, x_test = x_val, y_test = y_val, args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20PHKbJYhDtm"
      },
      "outputs": [],
      "source": [
        "model2, eval_model2 = CapsNet(input_shape=x_train.shape[1:], n_class = 30, routings = args.routings)\n",
        "model2.compile(optimizer=optimizers.Adam(learning_rate=args.lr),\n",
        "                  loss=['mse'])\n",
        "model2.load_weights('/content/result/tr_cp/imp_tr.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBWmeL1fA4NL",
        "outputId": "0d485ebb-6a6d-466f-f39c-8669b6fc4a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 11s 130ms/step\n"
          ]
        }
      ],
      "source": [
        "x_recon1 = model2.predict(x_train, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-Wsebk-wco",
        "outputId": "78383bae-d322-4c79-f10f-7200a025d2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 134ms/step\n"
          ]
        }
      ],
      "source": [
        "x_recon2 = model2.predict(x_test, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p21u9GV6-yQJ",
        "outputId": "36ae711d-c18a-4802-fe16-fd12be04ffb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 135ms/step\n"
          ]
        }
      ],
      "source": [
        "x_recon3 = model2.predict(x_val, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP1i7BqJ_AB9",
        "outputId": "1591da2c-64d7-4283-cdda-055eae888735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 2s 137ms/step - loss: 6.1020e-04\n"
          ]
        }
      ],
      "source": [
        "mse_val = model2.evaluate(x = x_val,y = y_val, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JT4sP7eAE39",
        "outputId": "8f98d08b-583f-4455-b439-62cd0c28232c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 138ms/step - loss: 6.5371e-04\n"
          ]
        }
      ],
      "source": [
        "mse_test = model2.evaluate(x = x_test,y = y_test, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhZ_x8SPAH7v",
        "outputId": "c730fed4-77ad-4679-a4f9-b068a89be102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 3s 134ms/step - loss: 1.1958e-04\n"
          ]
        }
      ],
      "source": [
        "mse_train = model2.evaluate(x = x_train,y = y_train, batch_size=50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
